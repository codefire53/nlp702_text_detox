Starting......................
train:
  do_train: true
  evaluation_strategy: epoch
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  learning_rate: 2.0e-05
  num_train_epochs: 60
  logging_strategy: epoch
  save_strategy: epoch
  save_total_limit: 1
  seed: 42
  gradient_accumulation_steps: 8
  load_best_model_at_end: true
  remove_unused_columns: true
  metric_for_best_model: eval_loss
models:
  trust_remote_code: true
  pretrained_model_name_or_path: google/mt5-base
madx:
  task_adapter_name: paradetox
  source_language: en
  source_adapter: en/wiki@ukp
  target_language: ru
  target_adapter: ru/wiki@ukp
tokenizers:
  add_special_tokens: true
  padding: max_length
  truncation: true
  max_length: 512
dataset:
  dataset_name: s-nlp/paradetox
  test_size: 0.1
  use_split: true
  use_json: false
  train_dataset_file: ./dataset/paradetox_train.json
  val_dataset_file: ./dataset/paradetox_val.json
  source: en_toxic_comment
  target: en_neutral_comment
collators:
  padding: max_length
earlystopping:
  early_stopping_patience: 3
adapters:
  name: unipelt
output_dir: ./outputs/mt5-base_unipelt_paradetox/checkpoints
logging_dir: ./outputs/mt5-base_unipelt_paradetox/logs
run_name: mt5-base_unipelt_paradetox

[2024-02-17 11:20:01,614][adapters.heads.base][INFO] - Adding head 'default' with config {'head_type': 'seq2seq_lm', 'vocab_size': 250112, 'layers': 1, 'activation_function': None, 'layer_norm': False, 'bias': False, 'shift_labels': False, 'label2id': None}.
[2024-02-17 11:20:02,851][adapters.configuration.model_adapters_config][INFO] - Adding adapter 'unipelt'.
[2024-02-17 11:20:03,811][adapters.heads.base][INFO] - Adding head 'unipelt' with config {'head_type': 'seq2seq_lm', 'vocab_size': 250112, 'layers': 1, 'activation_function': None, 'layer_norm': False, 'bias': False, 'shift_labels': False, 'label2id': None}.
Total trainable params: 520 params
