train:
  do_train: true
  evaluation_strategy: epoch
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  learning_rate: 2.0e-05
  num_train_epochs: 60
  logging_strategy: epoch
  save_strategy: epoch
  save_total_limit: 1
  seed: 42
  gradient_accumulation_steps: 8
  load_best_model_at_end: true
  remove_unused_columns: true
  metric_for_best_model: eval_loss
models:
  trust_remote_code: true
  pretrained_model_name_or_path: google/mt5-base
madx:
  task_adapter_name: paradetox
  source_language: en
  source_adapter: en/wiki@ukp
  target_language: ru
  target_adapter: ru/wiki@ukp
tokenizers:
  add_special_tokens: true
  padding: max_length
  truncation: true
  max_length: 512
dataset:
  dataset_name: s-nlp/paradetox
  test_size: 0.1
  use_split: true
  use_json: false
  train_dataset_file: ./dataset/paradetox_train.json
  val_dataset_file: ./dataset/paradetox_val.json
  source: en_toxic_comment
  target: en_neutral_comment
collators:
  padding: max_length
earlystopping:
  early_stopping_patience: 3
adapters:
  name: paradetox_pfeiffer
  non_linearity: relu
  reduction_factor: 2
output_dir: ./outputs/mt5-base_madx/checkpoints
logging_dir: ./outputs/mt5-base_madx/logs
run_name: mt5-base_madx
